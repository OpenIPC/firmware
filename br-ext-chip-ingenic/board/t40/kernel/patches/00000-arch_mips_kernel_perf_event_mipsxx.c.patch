diff -drupN a/arch/mips/kernel/perf_event_mipsxx.c b/arch/mips/kernel/perf_event_mipsxx.c
--- a/arch/mips/kernel/perf_event_mipsxx.c	2017-10-21 18:09:07.000000000 +0300
+++ b/arch/mips/kernel/perf_event_mipsxx.c	2022-06-09 05:02:27.000000000 +0300
@@ -551,12 +551,33 @@ static atomic_t active_events = ATOMIC_I
 static DEFINE_MUTEX(pmu_reserve_mutex);
 static int (*save_perf_irq)(void);
 
+static void enable_interrupt(void *args)
+{
+	enable_percpu_irq(mipspmu.irq, IRQ_TYPE_LEVEL_HIGH);
+}
+static void disable_interrupt(void *args)
+{
+	disable_percpu_irq(mipspmu.irq);
+}
 static int mipspmu_get_irq(void)
 {
 	int err;
 
 	if (mipspmu.irq >= 0) {
 		/* Request my own irq handler. */
+#ifdef CONFIG_MACH_XBURST2
+		/* on ingenic xburst2 cpu, should request percpu interrupt. */
+		err = request_percpu_irq(mipspmu.irq, mipsxx_pmu_handle_irq,
+				"mips_perf_pmu", &mipspmu);
+
+		if (err) {
+			pr_warning("Unable to request IRQ%d for MIPS "
+			   "performance counters!\n", mipspmu.irq);
+		} else {
+			/* enable all cpus interrtup here. */
+			on_each_cpu(enable_interrupt, NULL, 1);
+		}
+#else
 		err = request_irq(mipspmu.irq, mipsxx_pmu_handle_irq,
 				  IRQF_PERCPU | IRQF_NOBALANCING |
 				  IRQF_NO_THREAD | IRQF_NO_SUSPEND |
@@ -566,6 +587,8 @@ static int mipspmu_get_irq(void)
 			pr_warn("Unable to request IRQ%d for MIPS performance counters!\n",
 				mipspmu.irq);
 		}
+
+#endif
 	} else if (cp0_perfcount_irq < 0) {
 		/*
 		 * We are sharing the irq number with the timer interrupt.
@@ -583,9 +606,15 @@ static int mipspmu_get_irq(void)
 
 static void mipspmu_free_irq(void)
 {
-	if (mipspmu.irq >= 0)
-		free_irq(mipspmu.irq, &mipspmu);
-	else if (cp0_perfcount_irq < 0)
+	if (mipspmu.irq >= 0) {
+#ifdef CONFIG_MACH_XBURST2
+		/* disable interrupt on all cpus */
+		on_each_cpu(disable_interrupt, NULL, 1);
+		free_percpu_irq(mipspmu.irq, &mipspmu);
+#else
+		free_irq(mipspmu.irq, NULL);
+#endif
+	} else if (cp0_perfcount_irq < 0)
 		perf_irq = save_perf_irq;
 }
 
@@ -858,6 +887,13 @@ static const struct mips_perf_event xlp_
 	[PERF_COUNT_HW_BRANCH_MISSES] = { 0x1c, CNTR_ALL }, /* PAPI_BR_MSP */
 };
 
+static const struct mips_perf_event ingenic_event_map[PERF_COUNT_HW_MAX] = {
+	[PERF_COUNT_HW_CPU_CYCLES] = { 0x00, CNTR_EVEN | CNTR_ODD, P },
+	[PERF_COUNT_HW_INSTRUCTIONS] = { 0x01, CNTR_EVEN | CNTR_ODD, T },
+	[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = { 0x02, CNTR_EVEN, T },
+	[PERF_COUNT_HW_BRANCH_MISSES] = { 0x02, CNTR_ODD, T },
+};
+
 /* 24K/34K/1004K/interAptiv/loongson1 cores share the same cache event map. */
 static const struct mips_perf_event mipsxxcore_cache_map
 				[PERF_COUNT_HW_CACHE_MAX]
@@ -1227,6 +1263,45 @@ static const struct mips_perf_event xlp_
 },
 };
 
+static const struct mips_perf_event ingenic_cache_map
+				[PERF_COUNT_HW_CACHE_MAX]
+				[PERF_COUNT_HW_CACHE_OP_MAX]
+				[PERF_COUNT_HW_CACHE_RESULT_MAX] = {
+[C(L1D)] = {
+	/*
+	 * Like some other architectures (e.g. ARM), the performance
+	 * counters don't differentiate between read and write
+	 * accesses/misses, so this isn't strictly correct, but it's the
+	 * best we can do. Writes and reads get combined.
+	 */
+	[C(OP_READ)] = {
+		[C(RESULT_ACCESS)]	= { 0x04, CNTR_EVEN, T },
+		[C(RESULT_MISS)]	= { 0x04, CNTR_ODD, T },
+	},
+	[C(OP_WRITE)] = {
+		[C(RESULT_ACCESS)]	= { 0x05, CNTR_EVEN, T },
+		[C(RESULT_MISS)]	= { 0x05, CNTR_ODD, T },
+	},
+},
+[C(L1I)] = {
+	[C(OP_READ)] = {
+		[C(RESULT_ACCESS)]	= { 0x07, CNTR_EVEN, T },
+		[C(RESULT_MISS)]	= { 0x07, CNTR_ODD, T },
+	},
+},
+[C(DTLB)] = {
+	[C(OP_READ)] = {
+		[C(RESULT_ACCESS)]	= { 0x09, CNTR_EVEN, T },
+		[C(RESULT_MISS)]	= { 0x09, CNTR_ODD, T },
+	},
+},
+[C(ITLB)] = {
+	[C(OP_READ)] = {
+		[C(RESULT_ACCESS)]	= { 0x08, CNTR_EVEN, T },
+		[C(RESULT_MISS)]	= { 0x08, CNTR_ODD, T },
+	},
+},
+};
 #ifdef CONFIG_MIPS_MT_SMP
 static void check_and_calc_range(struct perf_event *event,
 				 const struct mips_perf_event *pev)
@@ -1767,6 +1842,11 @@ init_hw_perf_events(void)
 		mipspmu.cache_event_map = &xlp_cache_map;
 		mipspmu.map_raw_event = xlp_pmu_map_raw_event;
 		break;
+	case CPU_JZRISC:
+		mipspmu.name = "ingenic";
+		mipspmu.general_event_map = &ingenic_event_map;
+		mipspmu.cache_event_map = &ingenic_cache_map;
+		break;
 	default:
 		pr_cont("Either hardware does not support performance "
 			"counters, or not yet implemented.\n");
